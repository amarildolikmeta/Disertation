\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{Sutton:1998:IRL:551283}
\citation{Sutton:1998:IRL:551283}
\citation{Dearden98bayesianq-learning}
\citation{DBLP:journals/corr/OsbandBPR16}
\citation{DBLP:journals/corr/BellemareDM17}
\citation{Dearden98bayesianq-learning}
\citation{Dearden98bayesianq-learning}
\citation{Dearden98bayesianq-learning}
\citation{Strehl2008AnAO}
\citation{Strehl2008AnAO}
\citation{DBLP:conf/icml/FruitPLO18}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Watkins:89}
\citation{rummery:cuedtr94}
\citation{Sutton:1998:IRL:551283}
\citation{Watkins:89}
\citation{rummery:cuedtr94}
\citation{mnih2015humanlevel}
\citation{DBLP:journals/corr/OsbandBPR16}
\citation{mnih2015humanlevel}
\citation{mnih2015humanlevel}
\citation{Kearns:2002:NRL:599616.599699}
\citation{Brafman:2003:RGP:944919.944928}
\citation{NIPS2006_3052}
\citation{Strehl:2006:PMR:1143844.1143955}
\citation{Dearden98bayesianq-learning}
\citation{Kearns:2002:NRL:599616.599699}
\citation{Brafman:2003:RGP:944919.944928}
\citation{NIPS2006_3052}
\citation{Strehl:2006:PMR:1143844.1143955}
\citation{DBLP:journals/corr/OsbandBPR16}
\citation{DBLP:journals/corr/BellemareDM17}
\citation{DBLP:journals/corr/abs-1710-10044}
\@writefile{toc}{\contentsline {subsubsection}{Goal}{3}{section*.8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Overview}{3}{section.1.1}}
\citation{DBLP:journals/corr/OsbandBPR16}
\citation{Sutton:1998:IRL:551283}
\citation{Sutton:1998:IRL:551283}
\citation{hastie_09_elements-of.statistical-learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reinforcement Learning and Markov Decision Processes}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:chapter2}{{2}{5}{Reinforcement Learning and Markov Decision Processes}{chapter.2}{}}
\citation{Silver_2016}
\citation{silver2017mastering}
\citation{hastie_09_elements-of.statistical-learning}
\citation{Sutton:1998:IRL:551283}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Reinforcement Learning framework~\cite  {Sutton:1998:IRL:551283}.\relax }}{6}{figure.caption.9}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:rl_framework}{{2.1}{6}{The Reinforcement Learning framework~\cite {Sutton:1998:IRL:551283}.\relax }{figure.caption.9}{}}
\citation{Dearden98bayesianq-learning}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Markov Decision Processes}{7}{section.2.1}}
\newlabel{MDPs}{{2.1}{7}{Markov Decision Processes}{section.2.1}{}}
\citation{Sutton:1998:IRL:551283}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Definitions}{8}{subsection.2.1.1}}
\citation{Sutton:1998:IRL:551283}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Policies}{9}{subsection.2.1.2}}
\citation{Puterman:1994:MDP:528623}
\citation{Sutton:1998:IRL:551283}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Utility Functions and Value Functions}{10}{subsection.2.1.3}}
\citation{Sutton:1998:IRL:551283}
\citation{Sutton:1998:IRL:551283}
\citation{LeemonCBaird93}
\citation{SchulmanMLJA15}
\citation{Bellman:DynamicProgramming}
\citation{Bellman:DynamicProgramming}
\citation{Puterman:1994:MDP:528623}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Bellman Equations and Operators}{12}{subsection.2.1.4}}
\newlabel{eq:mdp_closed_form_solution}{{2.12}{12}{Bellman Expectation Equation}{equation.2.1.12}{}}
\citation{Sutton:1998:IRL:551283}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Optimality Conditions}{13}{subsection.2.1.5}}
\citation{Sutton:1998:IRL:551283}
\citation{doi:10.2200/S00426ED1V01Y201206AIM017}
\@writefile{toc}{\contentsline {subsubsection}{Bellman Optimality Equation}{14}{section*.12}}
\newlabel{eq:bellman_optimality}{{2.18}{14}{}{equation.2.1.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{Optimal Policies}{14}{section*.13}}
\citation{Bellman:DynamicProgramming}
\citation{howard:dynamic60}
\citation{Sutton:1998:IRL:551283}
\citation{Sutton:1998:IRL:551283}
\citation{Puterman:1978:MPI:2828482.2828486}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Planning in MDPs}{15}{section.2.2}}
\newlabel{planning_mdps}{{2.2}{15}{Planning in MDPs}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Dynamic Programming}{15}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Policy Iteration}{15}{subsection.2.2.2}}
\citation{Banach1922}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Policy iteration algorithm~\cite  {Sutton:1998:IRL:551283}.\relax }}{16}{figure.caption.14}}
\newlabel{fig:policy_iteration}{{2.2}{16}{Policy iteration algorithm~\cite {Sutton:1998:IRL:551283}.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Value Iteration}{16}{subsection.2.2.3}}
\citation{DBLP:journals/corr/abs-1301-6718}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning}{17}{section.2.3}}
\newlabel{reinforcement_learning}{{2.3}{17}{Reinforcement Learning}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Classifying Reinforcement Learning Algorithms}{17}{subsection.2.3.1}}
\citation{Sutton:1998:IRL:551283}
\citation{Sutton:1998:IRL:551283}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Model-free Prediction}{18}{subsection.2.3.2}}
\citation{Sutton:1998:IRL:551283}
\citation{Sutton:1998:IRL:551283}
\citation{Sutton:1998:IRL:551283}
\citation{rummery:cuedtr94}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Model-free Control}{19}{subsection.2.3.3}}
\citation{Jaakkola:1994:CSI:1362288.1362296}
\citation{Sutton:1998:IRL:551283}
\citation{Watkins:89}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces $Q$-learning: Learn function $Q: \mathcal  {S} \times \mathcal  {A} \rightarrow \mathbb  {R}$\relax }}{20}{algorithm.1}}
\newlabel{alg:q-learning}{{1}{20}{$Q$-learning: Learn function $Q: \mathcal {S} \times \mathcal {A} \rightarrow \mathbb {R}$\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Function Approximation}{20}{section.2.4}}
\newlabel{sec:function_approximation}{{2.4}{20}{Function Approximation}{section.2.4}{}}
\citation{hastie01statisticallearning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Basics of Function Approximation}{21}{subsection.2.4.1}}
\newlabel{eq:loss_function_func_approximator}{{2.27}{21}{Basics of Function Approximation}{equation.2.4.27}{}}
\citation{Sutton:1998:IRL:551283}
\citation{DBLP:journals/corr/BellemareDM17}
\citation{jaquette1973}
\citation{articleSobel}
\citation{Morimura:2010:NRD:3104322.3104424}
\citation{Dearden98bayesianq-learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A frame from the video game Space Invaders.\relax }}{22}{figure.caption.15}}
\newlabel{fig:space_invaders}{{2.3}{22}{A frame from the video game Space Invaders.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Distributional Reinforcement Learning}{22}{section.2.5}}
\newlabel{distributional_rl_section}{{2.5}{22}{Distributional Reinforcement Learning}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Value Distributions}{22}{subsection.2.5.1}}
\citation{Kullback59}
\citation{BurnhamModelSelection}
\citation{bootstrapAsymptotic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Distribution Distance Measures}{23}{subsection.2.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{KL Divergence}{23}{section*.16}}
\@writefile{toc}{\contentsline {subsubsection}{Wasserstein Metric}{23}{section*.17}}
\citation{DBLP:journals/corr/BellemareDM17}
\citation{DBLP:journals/corr/BellemareDM17}
\citation{DBLP:journals/corr/BellemareDM17}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Policy Evaluation Setting}{24}{subsection.2.5.3}}
\newlabel{eq:bellman_distributional}{{2.34}{24}{Policy Evaluation Setting}{equation.2.5.34}{}}
\citation{pmlr-v70-arjovsky17a}
\citation{DBLP:journals/corr/BellemareDM17}
\citation{DBLP:journals/corr/BellemareDM17}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}The Control Setting}{25}{subsection.2.5.4}}
\citation{Thrun92efficientexploration}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}State of The Art in Efficient Exploration}{27}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:chapter3}{{3}{27}{State of The Art in Efficient Exploration}{chapter.3}{}}
\citation{Gatsby2003OnTS}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Exploration}{28}{section.3.1}}
\newlabel{exploration}{{3.1}{28}{Exploration}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Measuring Efficient Exploration}{28}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{Sample Complexity}{28}{section*.18}}
\@writefile{toc}{\contentsline {subsubsection}{Regret}{28}{section*.19}}
\citation{Kearns:2002:NRL:599616.599699}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Explicit Explore or Exploit}{29}{subsection.3.1.2}}
\citation{Kearns:2002:NRL:599616.599699}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces $E^3$ algorithm\relax }}{30}{algorithm.2}}
\newlabel{alg:e3}{{2}{30}{$E^3$ algorithm\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}R-Max}{30}{subsection.3.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Games}{30}{section*.20}}
\citation{KLMSurvey}
\citation{Sutton:1998:IRL:551283}
\citation{journals/corr/abs-1204-5721}
\citation{kaelbling1993learning}
\citation{DBLP:journals/sigart/Sutton91}
\citation{Brafman:2003:RGP:944919.944928}
\@writefile{toc}{\contentsline {subsubsection}{R-max algorithm}{31}{section*.21}}
\citation{NIPS2006_3052}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Upper Confidence Reinforcement Learning}{32}{subsection.3.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{UCRL}{32}{section*.22}}
\citation{NIPS2006_3052}
\citation{Jaksch:2010:NRB:1756006.1859902}
\@writefile{toc}{\contentsline {subsubsection}{UCRL2}{33}{section*.23}}
\citation{Jaksch:2010:NRB:1756006.1859902}
\citation{Strehl:2006:PMR:1143844.1143955}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Delayed Q Learning}{34}{subsection.3.1.5}}
\citation{Strehl:2006:PMR:1143844.1143955}
\citation{Dearden98bayesianq-learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Bayesian Q Learning}{35}{subsection.3.1.6}}
\citation{degroot2012probability}
\citation{degroot2012probability}
\citation{articleWyatt1997}
\citation{Dearden98bayesianq-learning}
\citation{Dearden98bayesianq-learning}
\citation{4082064}
\@writefile{toc}{\contentsline {subsubsection}{Action Selection}{36}{section*.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Examples of Q-value distributions of two actions for which Q-value sampling has the same exploration policy even though the payoff of exploration in (b) is higher than in (a) ~\cite  {Dearden98bayesianq-learning}.\relax }}{37}{figure.caption.25}}
\newlabel{fig:q_value_sampling}{{3.1}{37}{Examples of Q-value distributions of two actions for which Q-value sampling has the same exploration policy even though the payoff of exploration in (b) is higher than in (a) ~\cite {Dearden98bayesianq-learning}.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{Updating the Distribution}{37}{section*.26}}
\citation{degroot2012probability}
\newlabel{eq:moment_updating}{{3.10}{38}{Updating the Distribution}{equation.3.1.10}{}}
\citation{mnih2015humanlevel}
\citation{Tesauro:1995:TDL:203330.203343}
\citation{Osband:2016:GEV:3045390.3045641}
\citation{Osband2017WhyIP}
\citation{NIPS2013_5185}
\citation{DBLP:journals/corr/OsbandR15}
\citation{Osband2017DeepEV}
\citation{DBLP:journals/corr/OsbandBPR16}
\citation{mnih2015humanlevel}
\citation{Blundell:2015:WUN:3045118.3045290}
\citation{Gal:2016:DBA:3045390.3045502}
\citation{EfroTibs93}
\citation{bootstrapAsymptotic}
\citation{DBLP:journals/corr/OsbandBPR16}
\citation{DBLP:journals/corr/OsbandBPR16}
\citation{DBLP:journals/corr/OsbandBPR16}
\citation{mnih2015humanlevel}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Deep Exploration}{39}{section.3.2}}
\newlabel{deep_exploration}{{3.2}{39}{Deep Exploration}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Bootstrapped Q Learning}{39}{subsection.3.2.1}}
\citation{Goodfellow-et-al-2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Network architecture in Bootstrapped DQN ~\cite  {DBLP:journals/corr/OsbandBPR16}.\relax }}{40}{figure.caption.27}}
\newlabel{fig:BDQN_Architecture}{{3.2}{40}{Network architecture in Bootstrapped DQN ~\cite {DBLP:journals/corr/OsbandBPR16}.\relax }{figure.caption.27}{}}
\newlabel{eq:double_DQN_target}{{3.14}{40}{Bootstrapped Q Learning}{equation.3.2.14}{}}
\citation{Strens00abayesian}
\citation{NIPS2013_5185}
\citation{owen2012}
\citation{DBLP:journals/corr/BellemareDM17}
\citation{VanDenOord:2016:PRN:3045390.3045575}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Bootstrapped DQN\relax }}{42}{algorithm.3}}
\newlabel{alg:bootstrapped_dqn}{{3}{42}{Bootstrapped DQN\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Distributional RL}{42}{section.3.3}}
\newlabel{distributional_rl}{{3.3}{42}{Distributional RL}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Approximate Distributional Learning: C51}{42}{subsection.3.3.1}}
\citation{DBLP:journals/corr/BellemareDM17}
\citation{DBLP:journals/corr/BellemareDM17}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Visualization of the distributional Bellman operator ~\cite  {DBLP:journals/corr/BellemareDM17}.\relax }}{43}{figure.caption.29}}
\newlabel{fig:distributional_operator}{{3.3}{43}{Visualization of the distributional Bellman operator ~\cite {DBLP:journals/corr/BellemareDM17}.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{Projection step}{43}{section*.28}}
\citation{DBLP:journals/corr/BellemareDM17}
\citation{koenker2005quantile}
\citation{DBLP:journals/corr/abs-1710-10044}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces $C51$ algorithm\relax }}{44}{algorithm.4}}
\newlabel{alg:c51}{{4}{44}{$C51$ algorithm\relax }{algorithm.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Distributional Reinforcement Learning with Quantile Regression}{44}{subsection.3.3.2}}
\citation{DBLP:journals/corr/abs-1710-10044}
\citation{Tsitsiklis97ananalysis}
\citation{DBLP:journals/corr/abs-1710-10044}
\newlabel{eq:quantile_distribution}{{3.19}{45}{Distributional Reinforcement Learning with Quantile Regression}{equation.3.3.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Quantile Approximation}{45}{section*.30}}
\newlabel{lem:MinQuantileDist}{{3.3.1}{45}{}{lemma.3.3.1}{}}
\citation{huber:1964}
\citation{DBLP:journals/corr/abs-1710-10044}
\newlabel{eq:quantile_regression_loss}{{3.23}{46}{The Quantile Approximation}{equation.3.3.23}{}}
\newlabel{eq:quantile_huber_loss}{{3.25}{46}{The Quantile Approximation}{equation.3.3.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{QRTD}{46}{section*.31}}
\citation{mnih2015humanlevel}
\@writefile{toc}{\contentsline {subsubsection}{Quantile Regression DQN}{47}{section*.32}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces $QR-DQN$ algorithm\relax }}{47}{algorithm.5}}
\newlabel{alg:QRDQN}{{5}{47}{$QR-DQN$ algorithm\relax }{algorithm.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Particle Q Learning}{48}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:chapter4}{{4}{48}{Particle Q Learning}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Q Value Distributions}{48}{section.4.1}}
\newlabel{sec:q_value_distributions}{{4.1}{48}{Q Value Distributions}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Particle Q Distribution}{49}{subsection.4.1.1}}
\newlabel{eq:particle_distribution}{{4.2}{49}{Particle Q Distribution}{equation.4.1.2}{}}
\citation{NIPS2017_7149}
\citation{journals/siamma/AguehC11}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Behaviour of the Q-distribution over time.\relax }}{50}{figure.caption.33}}
\newlabel{fig:q_distribution_behaviour}{{4.1}{50}{Behaviour of the Q-distribution over time.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Sample Mean Distribution}{50}{subsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Approximation of a distribution function with mixture of delta functions}{51}{subsection.4.1.3}}
\citation{Steele:2004:CMC:993490}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Wasserstein barycenter}{53}{subsection.4.1.4}}
\newlabel{eq:problem}{{4.17}{53}{Wasserstein barycenter}{equation.4.1.17}{}}
\citation{Dearden98bayesianq-learning}
\citation{pmlr-v48-deramo16}
\citation{4082064}
\citation{Russell:1991:RTS:110787}
\citation{Dearden98bayesianq-learning}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Action Selection}{54}{section.4.2}}
\newlabel{sec:action_selection}{{4.2}{54}{Action Selection}{section.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Q-distributions of 3 different actions.\relax }}{55}{figure.caption.34}}
\newlabel{fig:q_distributions}{{4.2}{55}{Q-distributions of 3 different actions.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}VPI Policy}{55}{subsection.4.2.1}}
\citation{Dearden98bayesianq-learning}
\citation{Dearden98bayesianq-learning}
\citation{Dearden98bayesianq-learning}
\citation{pmlr-v48-deramo16}
\citation{Dearden98bayesianq-learning}
\newlabel{eq:vpi_equation}{{4.28}{57}{VPI Policy}{equation.4.2.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Weighted Policy}{57}{subsection.4.2.2}}
\newlabel{eq:maximum_probability}{{4.30}{57}{Weighted Policy}{equation.4.2.30}{}}
\citation{pmlr-v48-deramo16}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Updating the Q-distribution}{58}{section.4.3}}
\newlabel{sec:updating_q_distributions}{{4.3}{58}{Updating the Q-distribution}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Sorted Update}{58}{subsection.4.3.1}}
\newlabel{eq:sorted_update}{{4.32}{58}{Sorted Update}{equation.4.3.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Maximum Mean Updating}{58}{subsection.4.3.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Maximum Mean Updating\relax }}{58}{algorithm.6}}
\newlabel{alg:max_mean_updating}{{6}{58}{Maximum Mean Updating\relax }{algorithm.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Target Q-distributions of state $s'$.\relax }}{59}{figure.caption.35}}
\newlabel{fig:target_distributions}{{4.3}{59}{Target Q-distributions of state $s'$.\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Weighted Updating}{59}{subsection.4.3.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Weighted Updating\relax }}{60}{algorithm.7}}
\newlabel{alg:weighted_updating}{{7}{60}{Weighted Updating\relax }{algorithm.7}{}}
\citation{Bellemare:2013:ALE:2566972.2566979}
\citation{DBLP:journals/corr/OsbandBPR16}
\citation{Hasselt:2016:DRL:3016100.3016191}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experiments}{61}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:chapter5}{{5}{61}{Experiments}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Tabular Case}{61}{section.5.1}}
\newlabel{sec:tabular_experiments}{{5.1}{61}{Tabular Case}{section.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Evaluation Metrics}{61}{subsection.5.1.1}}
\citation{DBLP:journals/corr/OsbandBPR16}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Experimental Setup}{62}{subsection.5.1.2}}
\newlabel{eq:exponential_decay}{{5.1}{62}{Experimental Setup}{equation.5.1.1}{}}
\citation{Dearden98bayesianq-learning}
\citation{Dearden98bayesianq-learning}
\citation{Dearden98bayesianq-learning}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Chain domain taken from ~\cite  {Dearden98bayesianq-learning}.\relax }}{63}{figure.caption.36}}
\newlabel{fig:chain_domain}{{5.1}{63}{Chain domain taken from ~\cite {Dearden98bayesianq-learning}.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Chain Domain}{63}{subsection.5.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Online (left) and offline (right) scores in the Chain domain.\relax }}{64}{figure.caption.37}}
\newlabel{fig:chain_learning_curve}{{5.2}{64}{Online (left) and offline (right) scores in the Chain domain.\relax }{figure.caption.37}{}}
\newlabel{fig:chain_particles_weighted_weighted}{{5.3a}{65}{\relax }{figure.caption.38}{}}
\newlabel{sub@fig:chain_particles_weighted_weighted}{{a}{65}{\relax }{figure.caption.38}{}}
\newlabel{fig:chain_particles_vpi_mean}{{5.3b}{65}{\relax }{figure.caption.38}{}}
\newlabel{sub@fig:chain_particles_vpi_mean}{{b}{65}{\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Evolution of the particles in the first state of the Chain domain during the learning process for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b). The optimal action \emph  {a} is shown on the left, while the suboptimal action \emph  {b} is shown on the right on both (a) and (b).\relax }}{65}{figure.caption.38}}
\newlabel{fig:chain_particle_evolution}{{5.3}{65}{Evolution of the particles in the first state of the Chain domain during the learning process for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b). The optimal action \emph {a} is shown on the left, while the suboptimal action \emph {b} is shown on the right on both (a) and (b).\relax }{figure.caption.38}{}}
\newlabel{fig:chain_std_weighted_weighted}{{\caption@xref {fig:chain_std_weighted_weighted}{ on input line 68}}{66}{Chain Domain}{figure.caption.39}{}}
\newlabel{sub@fig:chain_std_weighted_weighted}{{}{66}{Chain Domain}{figure.caption.39}{}}
\newlabel{fig:chain_std_vpi_mean}{{\caption@xref {fig:chain_std_vpi_mean}{ on input line 77}}{66}{Chain Domain}{figure.caption.39}{}}
\newlabel{sub@fig:chain_std_vpi_mean}{{a}{66}{Chain Domain}{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Standard deviation of the particles as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b). Once again, the optimal action \emph  {a} is shown on the left, while the suboptimal action \emph  {b} is shown on the right on both (a) and (b).\relax }}{66}{figure.caption.39}}
\newlabel{fig:chain_std_evolution}{{5.4}{66}{Standard deviation of the particles as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b). Once again, the optimal action \emph {a} is shown on the left, while the suboptimal action \emph {b} is shown on the right on both (a) and (b).\relax }{figure.caption.39}{}}
\citation{Dearden98bayesianq-learning}
\citation{Dearden98bayesianq-learning}
\citation{Dearden98bayesianq-learning}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Probability of exploration as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update in the Chain domain.\relax }}{67}{figure.caption.40}}
\newlabel{fig:chain_prob_evolution}{{5.5}{67}{Probability of exploration as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update in the Chain domain.\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Loop Domain}{67}{subsection.5.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Loop domain taken from ~\cite  {Dearden98bayesianq-learning}.\relax }}{68}{figure.caption.41}}
\newlabel{fig:loop_domain}{{5.6}{68}{Loop domain taken from ~\cite {Dearden98bayesianq-learning}.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Online and offline scores in the Loop domain.\relax }}{68}{figure.caption.42}}
\newlabel{fig:loop_learning_curve}{{5.7}{68}{Online and offline scores in the Loop domain.\relax }{figure.caption.42}{}}
\citation{Dearden98bayesianq-learning}
\citation{Dearden98bayesianq-learning}
\citation{Dearden98bayesianq-learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Taxi Domain}{69}{subsection.5.1.5}}
\newlabel{fig:loop_particles_weighted_weighted}{{\caption@xref {fig:loop_particles_weighted_weighted}{ on input line 109}}{70}{Loop Domain}{figure.caption.43}{}}
\newlabel{sub@fig:loop_particles_weighted_weighted}{{}{70}{Loop Domain}{figure.caption.43}{}}
\newlabel{fig:loop_particles_vpi_mean}{{5.8b}{70}{\relax }{figure.caption.43}{}}
\newlabel{sub@fig:loop_particles_vpi_mean}{{b}{70}{\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Evolution of the particles in the first state of the Loop domain during the learning process for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b). The optimal action \emph  {b} is shown on the right, while the suboptimal action is shown on the left on both (a) and (b).\relax }}{70}{figure.caption.43}}
\newlabel{fig:loop_particle_evolution}{{5.8}{70}{Evolution of the particles in the first state of the Loop domain during the learning process for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b). The optimal action \emph {b} is shown on the right, while the suboptimal action is shown on the left on both (a) and (b).\relax }{figure.caption.43}{}}
\newlabel{fig:loop_std_weighted_weighted}{{\caption@xref {fig:loop_std_weighted_weighted}{ on input line 130}}{71}{Loop Domain}{figure.caption.44}{}}
\newlabel{sub@fig:loop_std_weighted_weighted}{{}{71}{Loop Domain}{figure.caption.44}{}}
\newlabel{fig:loop_std_vpi_mean}{{\caption@xref {fig:loop_std_vpi_mean}{ on input line 139}}{71}{Loop Domain}{figure.caption.44}{}}
\newlabel{sub@fig:loop_std_vpi_mean}{{a}{71}{Loop Domain}{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Standard deviation of the particles as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b) in the Loop domain. The optimal action is shown on the right, while the suboptimal action is shown on the left on both (a) and (b).\relax }}{71}{figure.caption.44}}
\newlabel{fig:loop_std_evolution}{{5.9}{71}{Standard deviation of the particles as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b) in the Loop domain. The optimal action is shown on the right, while the suboptimal action is shown on the left on both (a) and (b).\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Probability of exploration as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update in the Loop domain.\relax }}{72}{figure.caption.45}}
\newlabel{fig:loop_prob_evolution}{{5.10}{72}{Probability of exploration as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update in the Loop domain.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Taxi domain taken from ~\cite  {Dearden98bayesianq-learning}, where it appears as the Maze domain.\relax }}{72}{figure.caption.46}}
\newlabel{fig:taxi_domain}{{5.11}{72}{Taxi domain taken from ~\cite {Dearden98bayesianq-learning}, where it appears as the Maze domain.\relax }{figure.caption.46}{}}
\citation{Strehl2008AnAO}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Online and offline scores in the Taxi domain.\relax }}{73}{figure.caption.47}}
\newlabel{fig:taxi_learning_curve}{{5.12}{73}{Online and offline scores in the Taxi domain.\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.6}River Swim Domain }{73}{subsection.5.1.6}}
\citation{Strehl2008AnAO}
\citation{Strehl2008AnAO}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces River Swim domain taken from ~\cite  {Strehl2008AnAO}.\relax }}{74}{figure.caption.48}}
\newlabel{fig:riverswim_domain}{{5.13}{74}{River Swim domain taken from ~\cite {Strehl2008AnAO}.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Online and offline scores in the River Swim domain.\relax }}{74}{figure.caption.49}}
\newlabel{fig:riverswim_learning_curve}{{5.14}{74}{Online and offline scores in the River Swim domain.\relax }{figure.caption.49}{}}
\newlabel{fig:riverswim_particles_weighted_weighted}{{\caption@xref {fig:riverswim_particles_weighted_weighted}{ on input line 186}}{75}{River Swim Domain}{figure.caption.50}{}}
\newlabel{sub@fig:riverswim_particles_weighted_weighted}{{}{75}{River Swim Domain}{figure.caption.50}{}}
\newlabel{fig:riverswim_particles_vpi_mean}{{5.15b}{75}{\relax }{figure.caption.50}{}}
\newlabel{sub@fig:riverswim_particles_vpi_mean}{{b}{75}{\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Evolution of the particles in the first state of the RiverSwim domain during the learning process for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b). The optimal action is shown on the right, while the suboptimal action is shown on the left on both (a) and (b).\relax }}{75}{figure.caption.50}}
\newlabel{fig:riverswin_particle_evolution}{{5.15}{75}{Evolution of the particles in the first state of the RiverSwim domain during the learning process for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b). The optimal action is shown on the right, while the suboptimal action is shown on the left on both (a) and (b).\relax }{figure.caption.50}{}}
\citation{Strehl2008AnAO}
\citation{Strehl2008AnAO}
\citation{Strehl2008AnAO}
\citation{DBLP:conf/icml/FruitPLO18}
\citation{DBLP:conf/icml/FruitPLO18}
\citation{DBLP:conf/icml/FruitPLO18}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.7}Six Arms Domain}{76}{subsection.5.1.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.8}Knight Quest}{76}{subsection.5.1.8}}
\newlabel{fig:riverswim_std_weighted_weighted}{{\caption@xref {fig:riverswim_std_weighted_weighted}{ on input line 207}}{77}{River Swim Domain}{figure.caption.51}{}}
\newlabel{sub@fig:riverswim_std_weighted_weighted}{{}{77}{River Swim Domain}{figure.caption.51}{}}
\newlabel{fig:riverswim_std_vpi_mean}{{\caption@xref {fig:riverswim_std_vpi_mean}{ on input line 216}}{77}{River Swim Domain}{figure.caption.51}{}}
\newlabel{sub@fig:riverswim_std_vpi_mean}{{a}{77}{River Swim Domain}{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Standard deviation of the particles as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b) in the RiverSwim domain. The optimal action is shown on the right, while the suboptimal action is shown on the left on both (a) and (b).\relax }}{77}{figure.caption.51}}
\newlabel{fig:riverswim_std_evolution}{{5.16}{77}{Standard deviation of the particles as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update (a), and VPI policy and maximum mean update (b) in the RiverSwim domain. The optimal action is shown on the right, while the suboptimal action is shown on the left on both (a) and (b).\relax }{figure.caption.51}{}}
\newlabel{fig:riverswim_prob_weighted_weighted}{{\caption@xref {fig:riverswim_prob_weighted_weighted}{ on input line 226}}{78}{River Swim Domain}{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Probability of exploration as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update in the RiverSwim domain.\relax }}{78}{figure.caption.52}}
\newlabel{fig:riverswim_prob_evolution}{{5.17}{78}{Probability of exploration as a function of the learning timestep for the Particle Q-learning algorithm with weighted policy and weighted update in the RiverSwim domain.\relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces Six Arms domain taken from ~\cite  {Strehl2008AnAO}.\relax }}{78}{figure.caption.53}}
\newlabel{fig:sixarms_domain}{{5.18}{78}{Six Arms domain taken from ~\cite {Strehl2008AnAO}.\relax }{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Online and offline scores in the SixArms domain.\relax }}{79}{figure.caption.54}}
\newlabel{fig:sixarms_learning_curve}{{5.19}{79}{Online and offline scores in the SixArms domain.\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces Knight Quest domain taken from ~\cite  {DBLP:conf/icml/FruitPLO18}. The green cells are the three locations the dragon can move to.\relax }}{79}{figure.caption.55}}
\newlabel{fig:knightquest_domain}{{5.20}{79}{Knight Quest domain taken from ~\cite {DBLP:conf/icml/FruitPLO18}. The green cells are the three locations the dragon can move to.\relax }{figure.caption.55}{}}
\citation{DBLP:journals/corr/OsbandBPR16}
\citation{mnih2015humanlevel}
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces Online and offline scores in the KnightQuest domain.\relax }}{81}{figure.caption.56}}
\newlabel{fig:knightQuest_learning_curve}{{5.21}{81}{Online and offline scores in the KnightQuest domain.\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Atari Experiments}{81}{section.5.2}}
\newlabel{sec:atari_experiments}{{5.2}{81}{Atari Experiments}{section.5.2}{}}
\citation{Bellemare:2013:ALE:2566972.2566979}
\citation{DBLP:journals/corr/OsbandBPR16}
\citation{DBLP:journals/corr/OsbandBPR16}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Experimental Setup}{82}{subsection.5.2.1}}
\citation{Bellemare:2013:ALE:2566972.2566979}
\@writefile{toc}{\contentsline {subsubsection}{Network Initialization}{83}{section*.57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Breakout}{83}{subsection.5.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Montezuma's Revenge}{83}{subsection.5.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces Frame taken from the Breakout Atari 2600 game.\relax }}{84}{figure.caption.58}}
\newlabel{fig:breakout_frame}{{5.22}{84}{Frame taken from the Breakout Atari 2600 game.\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.23}{\ignorespaces Evaluation scores in the Breakout Atari 2600 game.\relax }}{84}{figure.caption.59}}
\newlabel{fig:breakout_learning_curve}{{5.23}{84}{Evaluation scores in the Breakout Atari 2600 game.\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.24}{\ignorespaces Image of the first room of Montezuma's Revenge.\relax }}{85}{figure.caption.60}}
\newlabel{fig:montezuma_frame}{{5.24}{85}{Image of the first room of Montezuma's Revenge.\relax }{figure.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.25}{\ignorespaces Evaluation scores in Montezuma's Revenge.\relax }}{86}{figure.caption.61}}
\newlabel{fig:montezuma_learning_curve}{{5.25}{86}{Evaluation scores in Montezuma's Revenge.\relax }{figure.caption.61}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions}{87}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:chapter6}{{6}{87}{Conclusions}{chapter.6}{}}
\bibstyle{plain}
\bibdata{sample}
\@writefile{toc}{\contentsline {subsubsection}{Future Work}{88}{section*.62}}
\bibcite{journals/siamma/AguehC11}{1}
\bibcite{pmlr-v70-arjovsky17a}{2}
\bibcite{NIPS2006_3052}{3}
\bibcite{Banach1922}{4}
\bibcite{DBLP:journals/corr/BellemareDM17}{5}
\bibcite{Bellemare:2013:ALE:2566972.2566979}{6}
\bibcite{Bellman:DynamicProgramming}{7}
\bibcite{Blundell:2015:WUN:3045118.3045290}{8}
\bibcite{Brafman:2003:RGP:944919.944928}{9}
\bibcite{journals/corr/abs-1204-5721}{10}
\bibcite{BurnhamModelSelection}{11}
\bibcite{DBLP:journals/corr/abs-1710-10044}{12}
\bibcite{Dearden98bayesianq-learning}{13}
\bibcite{degroot2012probability}{14}
\bibcite{pmlr-v48-deramo16}{15}
\bibcite{EfroTibs93}{16}
\bibcite{DBLP:conf/icml/FruitPLO18}{17}
\bibcite{Gal:2016:DBA:3045390.3045502}{18}
\bibcite{Gatsby2003OnTS}{19}
\bibcite{Goodfellow-et-al-2016}{20}
\bibcite{Hasselt:2016:DRL:3016100.3016191}{21}
\bibcite{hastie01statisticallearning}{22}
\bibcite{hastie_09_elements-of.statistical-learning}{23}
\bibcite{4082064}{24}
\bibcite{howard:dynamic60}{25}
\bibcite{huber:1964}{26}
\bibcite{LeemonCBaird93}{27}
\bibcite{bootstrapAsymptotic}{28}
\bibcite{Jaakkola:1994:CSI:1362288.1362296}{29}
\bibcite{Jaksch:2010:NRB:1756006.1859902}{30}
\bibcite{jaquette1973}{31}
\bibcite{KLMSurvey}{32}
\bibcite{kaelbling1993learning}{33}
\bibcite{Kearns:2002:NRL:599616.599699}{34}
\bibcite{koenker2005quantile}{35}
\bibcite{Kullback59}{36}
\bibcite{NIPS2017_7149}{37}
\bibcite{DBLP:journals/corr/abs-1301-6718}{38}
\bibcite{doi:10.2200/S00426ED1V01Y201206AIM017}{39}
\bibcite{mnih2015humanlevel}{40}
\bibcite{Morimura:2010:NRD:3104322.3104424}{41}
\bibcite{DBLP:journals/corr/OsbandBPR16}{42}
\bibcite{DBLP:journals/corr/OsbandR15}{43}
\bibcite{Osband2017WhyIP}{44}
\bibcite{NIPS2013_5185}{45}
\bibcite{Osband2017DeepEV}{46}
\bibcite{Osband:2016:GEV:3045390.3045641}{47}
\bibcite{owen2012}{48}
\bibcite{Puterman:1994:MDP:528623}{49}
\bibcite{Puterman:1978:MPI:2828482.2828486}{50}
\bibcite{rummery:cuedtr94}{51}
\bibcite{Russell:1991:RTS:110787}{52}
\bibcite{SchulmanMLJA15}{53}
\bibcite{Silver_2016}{54}
\bibcite{silver2017mastering}{55}
\bibcite{articleSobel}{56}
\bibcite{Steele:2004:CMC:993490}{57}
\bibcite{Strehl:2006:PMR:1143844.1143955}{58}
\bibcite{Strehl2008AnAO}{59}
\bibcite{Strens00abayesian}{60}
\bibcite{DBLP:journals/sigart/Sutton91}{61}
\bibcite{Sutton:1998:IRL:551283}{62}
\bibcite{Tesauro:1995:TDL:203330.203343}{63}
\bibcite{Thrun92efficientexploration}{64}
\bibcite{Tsitsiklis97ananalysis}{65}
\bibcite{VanDenOord:2016:PRN:3045390.3045575}{66}
\bibcite{Watkins:89}{67}
\bibcite{articleWyatt1997}{68}
\citation{Hasselt:2016:DRL:3016100.3016191}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Experiments With Double Agents}{95}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsubsection}{Chain}{95}{section*.64}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Online (left) and offline (right) scores for the double algorithms in the Chain domain.\relax }}{95}{figure.caption.65}}
\@writefile{toc}{\contentsline {subsubsection}{Loop}{96}{section*.66}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Online (left) and offline (right) scores for the double algorithms in the Loop domain.\relax }}{96}{figure.caption.67}}
\@writefile{toc}{\contentsline {subsubsection}{Taxi}{96}{section*.68}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Online (left) and offline (right) scores for the double algorithms in the Taxi domain.\relax }}{96}{figure.caption.69}}
\@writefile{toc}{\contentsline {subsubsection}{River Swim}{97}{section*.70}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Online (left) and offline (right) scores for the double algorithms in the River Swim domain.\relax }}{97}{figure.caption.71}}
\@writefile{toc}{\contentsline {subsubsection}{Six Arms}{97}{section*.72}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces Online (left) and offline (right) scores for the double algorithms in the Six Arms domain.\relax }}{97}{figure.caption.73}}
\@writefile{toc}{\contentsline {subsubsection}{Knight Quest}{98}{section*.74}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.6}{\ignorespaces Online (left) and offline (right) scores for the double algorithms in the Knight Quest domain.\relax }}{98}{figure.caption.75}}
\newlabel{app:appendixA}{{A}{98}{Knight Quest}{figure.caption.75}{}}
