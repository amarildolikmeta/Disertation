Technology is continuously growing in terms of complexity and capabilities, whereas society is evolving and increasingly relying on computers to make decisions and to take actions on its behalf. Next-generation computer systems, too complex to manage for humans , will need to configure, optimize, and repair themselves. Self-driving cars are already on our roads, relieving humans of the monotony of driving during rush hours. Robots will accept tasks that are too dangerous for humans, from fighting fires to exploring disaster areas. These are just a few examples of \emph{autonomous agents} taking actions without human intervention to achieve some task. A key research question is how to make these agents’ behaviors robust like human decision-making. \par
Determining fixed behaviors in advance yields agents that fail to adapt to the inevitable unforeseen situations and uncertainty that characterize real-world tasks. To replicate human expertise, agents must replicate the human ability to learn and adapt. These \emph{learning} agents choose future behaviors in response to past behavior data. The learning agent needs to account for the delayed effect of its
decisions, along with the possible non-determinism of the environment. Clearly, in
order to plan a good decision, the agent must refer to some notion of \emph{optimality}. In real-life, optimality is not necessarily expressed in a formal way, however, when
a sequential decision-making problem is tackled from the point of view of Artificial
Intelligence (AI) we need to specify some notion of \emph{goal}. A \emph{goal-directed agent} is able not only to react to the changes of the environment, but also to show a proactive behavior, intended to achieve its goal. Differently from classic optimal control, no assumption is made on the knowledge of the dynamics of the environment, so the controller cannot be designed a priori, but needs to be learned from the interaction with the environment.\par
We will consider those problems that satisfy the Markov property, \ie the next state of the environment does not depend on the past states and actions given the current state and action, as well as the decision of the agent is determined as a function of the current environment state only. This requirement, while apparently unrealistic, can always be assumed provided that the representation of the environment state is
sufficiently rich (e.g., we can define a rich state as the concatenation of all the states and actions visited so far; in this way we embed the whole history in a single state). Such problems can be cast into the Markov Decision Processes (MDP) framework. The goal of learning agents in this environment is usually to maximize some sort of  \emph{utility function}. Whenever the agent performs an action, it receives a feedback from the environment, the \emph{immediate reward}, which depends, in the most general case, on the starting state, the action performed and the landing state. The utility, from a given state, is defined as the \emph{cumulative reward}, \ie the sum (possibly discounted) of the rewards collected along the visited states.\par
Reinforcement learning (RL) provides an appealing framework for developing learning agents. It grew out of promising early algorithms ~\cite{Watkins:89,rummery:cuedtr94} that guarantee convergence to optimal behavior in arbitrary finite agent-environment systems. These algorithms achieve these theoretical guarantees by estimating the long-term value of every state in the system. However, convergence to the correct values requires an infinite amount of data from each state (asymptotic convergence) and, in practice, these methods are rarely feasible in realistic applications. RL research has focused on scaling these methods on harder problems, working towards the goal of a relatively simple learning algorithm that allows an agent to cope with the complexity of the real world.\par
Interesting problems, especially tasks that are easy solvable by humans, suffer from the \emph{curse of dimensionality} in the sense that the state space size is usually exponential in the number of dimensions. It is thus not possible to brute force all the possible combinations of actions and save the best. There is an increasing need for smart strategies on how to explore the environment and decide when the agent should explore.
The natural counterpart of exploration is exploitation.  Both together form one of the fundamental challenges in RL: the Exploration vs. Exploitation dilemma~\cite{Sutton:1998:IRL:551283}. If some knowledge is obtained about the environment, the agent should use it to receive higher rewards. Logically pure random exploration is not enough to get high rewards. Therefore, imagine a human who has recently moved to a new city. During the first week he/she explores the city center by taking some randomly chosen routes through it. With this technique, he/she is going to find some spots, which match his/her personal interests. However if he/she decides to visit the best places, he/she found during the first week he/she might lose the opportunity to find other places, which are even better, because he/she stopped exploring the environment.\par
In order to solve the exploration vs. exploitation problem several strategies have been proposed. There are very simple ones, e.g., $\epsilon$-greedy strategies, used extensively in the literature in classic approaches ~\cite{Watkins:89,rummery:cuedtr94} as well as modern ones ~\cite{mnih2015humanlevel,DBLP:journals/corr/OsbandBPR16}. The drawback of these simple strategies is that they tend to need exponential many examples of states and actions and thus suffer from the curse of dimensionality as well. For example, take the Deep Q Network (DQN) architecture that was the first capable of playing Atari games at a human level ~\cite{mnih2015humanlevel}. Its actions are based solely on raw input pixel data, one of the main steps in designing a generally closed perception system.~\cite{mnih2015humanlevel} and perfomed on various Atari 2600 games well. However, a prominent example in which DQN fails is Montezuma’s Revenge. In this game, the player has to find several keys to unlock the doors unveiling the way to higher levels. The main reason for the malfunction is insufficient exploration in the regions of sparse rewards.\par
More advanced methods for exploration have been proposed in the literature. Some of them fall into the category of \emph{model-based} algorithms ~\cite{Kearns:2002:NRL:599616.599699,Brafman:2003:RGP:944919.944928,NIPS2006_3052}. They estimate the parameters of the underlying MPD model and use uncertainty about these estimates to drive exploration. \emph{Model-free} algorithms that perform efficient exploration have also been proposed ~\cite{Strehl:2006:PMR:1143844.1143955,Dearden98bayesianq-learning}. Some of these algorithms offer theoretical guarantees on the number of transitions they need to learn the optimal policies ~\cite{Kearns:2002:NRL:599616.599699,Brafman:2003:RGP:944919.944928,NIPS2006_3052,Strehl:2006:PMR:1143844.1143955}, but these theoretical bounds are either too lose, or apply only to small finite problems or both. Recent methods have been proposed to scale efficient and deep exploration in large problems, where function approximation (e.g., \emph{deep networks}) is used to generalize over the large state space ~\cite{DBLP:journals/corr/OsbandBPR16}. More recently, \emph{distributional reinforcement learning}, although it is not developed mainly to address the exploration problem, has been used to drive deep exploration in the context of \emph{deep reinforcement learning} ~\cite{DBLP:journals/corr/BellemareDM17,DBLP:journals/corr/abs-1710-10044}.
\par
This thesis continues in the tradition of scaling RL algorithms to harder tasks. We focus on the classical problem of balancing what is referred to as \emph{exploitation}: the estimation of optimal behavior given the existing data, with \emph{exploration}: the generation of behaviors intended to gather data that will improve future attempts at exploitation. In particular, most RL algorithms estimate optimal behavior from existing data while assuming that new data will not become available. They then modify this behavior to encourage the acquisition of new data, in the simplest case just by adding random actions. We advocate the development of agents, that explicitly  consider the uncertainty of their knowledge about the environment so that they can choose future behaviors in order to improve the usefulness of future data. By gathering more data, the agent improves its future ability to exploit the environment. This approach draws inspiration from human learning, which is most effective in the context of active experimentation, not passive observation. 
\subsubsection{Goal}
The goal of this thesis is to develop a model-free Reinforcement Learning algorithm that is able to perform efficient and deep exploration by explicitly representing the uncertainty  about its current Q-value estimates. 
\section{Overview}
The contents of this thesis are organized in the following five chapters. We start
in Chapter ~\ref{chap:chapter2} with an overview of the different aspects of Markov decision processes and reinforcement learning. We introduce definitions and discuss several aspects, such as value functions and optimal policies. Moreover, we outline the techniques for solving MDPs, starting with dynamic programming and presenting the most popular reinforcement learning approaches, along with  a brief discussion to function approximation methods. Finally, we give an overview of distributional reinforcement learning. The focus of the presentation is directed to the aspects that will be exploited in the subsequent chapters. Furthermore, in this chapter,
we introduce the notation that we will use throughout the thesis.\par
In Chapter ~\ref{chap:chapter3} we depict the landscape of the state-of-the-art algorithms in efficient exploration. The algorithms are presented in overlapping categories that group together methods that share similarities in the approach. For each category we mainly address a representative algorithm discussed in detail, emphasizing pros and cons. The goal of this chapter is to guide the reader in a conscious understanding of the fundamental motivations of this work.
Chapter ~\ref{chap:chapter4} is devoted to the extensive description of Particle Q-Learning. We begin by discussing our representation of the uncertainty of the agent using Q-distributions and present our main results about the approximation of the Q-distribution using a \emph{mixture of deltas} and how to update the distributions with new samples. Then we illustrate two policies that use the Q-distributions to make more informed decisions to balance exploration and exploitation. Finally, we discuss how to maintain our Q-distributions online, by using samples of the \emph{local immediate reward}.\par
In Chapter ~\ref{chap:chapter5} we evaluate Particle QL against popular RL methods. We first introduce the metrics we use to compare the performance of the algorithms. Then, for the considered domains, we analyze the performance in terms of \emph{learning speed} and we show how the Q-distributions shrink and how the uncertainty about the Q-values goes to 0. Besides the experiments in small domains, designed to require exploration, we evaluate our algorithm in the Atari suite of games, comparing our results with Bootstrapped DQN ~\cite{DBLP:journals/corr/OsbandBPR16} a recent deep RL algorithm that performs deep exploration. We test the algorithms in various Atari games, including the infamous Montezuma's Revenge.\par
In Chapter ~\ref{chap:chapter6} we summarize the most relevant achievements of this thesis and we highlight the strengths and weaknesses of the proposed approach. Furthermore, we suggest possible extensions of this work.
