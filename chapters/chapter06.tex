In this thesis we addressed the exploration vs. exploitation dilemma and proposed a new model-free algorithm that performs deep exploration by estimating the uncertainty about the agent's Q-values. We argued about the importance of maintaining Q-distributions to estimate this uncertainty, and further use these Q-distributions to drive exploration. We derived some interesting results on approximating the Q-distributions using the mixture of deltas model. We call these distributions Particle Q-Distributions. We compared our algorithm with some classical approaches to exploration as well as with Bootstrapped Q-learning, a state of the art algorithm that uses a similar approach to ours to drive exploration. We tested on various domains, some designed to encourage exploration as well as some simple navigation problems. Our algorithm showed improvements, sometimes substantial, in learning speed compared to Q-learning and Bootstrapped Q-learning when tested in finite MDPs. In the Taxi domain, not only we learned faster, but we also found the optimal policy of picking-up all the passengers, which Q-learning and Bootstrapped Q-learning failed to do.\par
We observed how the Q-distribution ``shrinked'' as the agent gathered more observations, driving the uncertainty about the agent's Q-values to zero. When using the weighted policy, this brought the probability of exploration to zero, and the agent exploits its current Q-values. Differently from classic exploration approaches, like $\epsilon$-greedy, we do not need to explicitly define a schedule of exploration that vanishes as the state-action counters increase. Our agent will stop exploration automatically when it is certain about its Q-value estimates.\par
A drawback of our algorithm is the increased spatial and computational complexity. Compared to classical Q-learning, we now need to store $M$ times more values, as we will store the Q-distributions instead of the Q-values. On the other hand, we have the same spatial complexity as Bootstrapped Q-learning. However, our algorithms come with a higher computational cost. Bootstrapped Q-learning does not have any substantial additional cost compared to classical Q-learning as it uses one Q-value estimate at a time (in the basic version). Our method, on the other hand, uses all estimates at any time to perform the updates, which gives us a linear complexity \wrt the number of particles (given that we do not sort the particles at every update). Additionally, we add the cost of the policies, which calculate the VPI or probability of being maximum at every timestep. This adds a substantial computational cost to our algorithms which we have not analyzed. Intuitively, this larger computational requirement is compensated by the faster convergence to the optimal policy.\par 
We extended the algorithm to the deep RL world by borrowing the architecture used  in Bootstrapped DQN. We tested two versions of the particle algorithms and Bootstrapped DQN in Atari 2600 games. We tried to initialize the networks in such a way that the output of the network represents the equally spaced particles in the interval $[q_{min},q_{max}]$ (same prior distribution we used in the tabular case). We did this by forcing this values in the last layer by inducing a bias. This unfortunately caused our agents not to be unstable and fail to learn as a result. Consequently, we decided to use the initialization of Bootstrapped DQN, \ie we initialized each head randomly.\par 
In Breakout we scored slightly better than Boot\_DQN and also improved the stability of the agent. The tests in Montezuma's Revenge showed some promising results but more work needs to be done. 
\subsubsection{Future Work}
We received some encouraging results about the learning speed of our particle algorithm. Despite these good experimental results, unfortunately we have not yet proved the ``efficiency'' of Particle Q-learning, \ie we do not have any bounds on the sample complexity or  regret. Something we leave for future work definitely is deriving these bounds. Moreover we have not done any analysis of the computational complexity of our particle algorithm. Computing the probabilities of each action to be maximal (in weighted policy or weighted update) comes with a high computational cost. We could approximate these probabilities by sampling but that would come with performance cost. We might try to balance these computational and performance costs. Finally, we leave to future work finding better ways to initialize the deep network when using our particle algorithm. Currently we initialize the network randomly. We might try to pre-train the network to initialize the heads in the desired interval. Furthermore, we did not do an analysis of the hyperparameters of the deep network. More work can be done in this aspect to improve learning speed and stability of the agent. 