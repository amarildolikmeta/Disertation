I vari problemi decisionali sequenziali sono un oggetto di studio dell'intelligenza artificiale. L'apprendimento per rinforzo, è un framework che affronta questi problemi mediante un approccio ``trial and error''. In questo framework un agente interagisce con un ambiente e raccoglie esperienza da queste interazioni che a sua volta viene utilizzata per trovare la politica ottima da eseguire. Un elemento centrale dell'apprendimento è il segnale di ricompensa (reward) che l'agente riceve dall'ambiente comunicando all'agente se alcuni stati sono desiderati o dovrebbero essere evitati. Si assume che questa ricompensa sia ricevuta immediatamente dopo ogni azione e l'obiettivo dell'agente è quello di massimizzare la ricompensa cumulativa raccolta durante la sua attività nell'ambiente. Definito in questo modo, la funzione reward specifica l'attività che deve essere appresa dall'agente. \par
Il dilemma exploration vs. exploitation (esplorazione  vs. sfruttamento) rimane un argomento principale in reinforcement learning. Il problema consiste nel bilanciare la massimizzazione della ricompensa usando le conoscenze acquisite al momento con l'esplorazione di nuove azioni per migliorare la conoscenza dell'ambiente. Tradizionalmente, l'esplorazione è stata esplicitamente incorporata negli algoritmi scegliendo occasionalmente le azioni in maniera casuale invece di fare affidamento sull'esperienza raccolta; tuttavia rimane una sfida importante nell'apprendimento rinforzato. Strategie di esplorazione comuni, come $\epsilon$-greedy, non riescono a condurre esplorazioni estese o profonde. Ciò non solo comporta la necessità di quantità di dati esponenziale per gli algoritmi, ma soprattutto potrebbe causare una convergenza prematura degli algoritmi a una politica subottima o potrebbe impedire del tutto la convergenza. \par
Tradizionalmente, l'apprendimento per rinforzo affronta questi problemi stimando la funzione di valore che quantifica quanto ``desiderabili'' siano gli stati (o le coppie di stati-azione nel caso di funzione azione-valore). Essendo che gli agenti interagiscono con un ambiente stocastico, la funzione valore è la ricompensa cumulativa attesa a lungo termine. Algoritmi classici, classificati come model-free, cercano la politica ottima senza stimare esplicitamente il modello dell'ambiente. Questi algoritmi usano solo le stime dei Q-value per scegliere  le azioni. All'inizio dell'apprendimento, questi valori sono incerti, quindi l'agente non può fare affidamento su di essi. Metodi classici di esplorazione aggiungono una probabilità di esplorazione che svanisce col tempo. Siccome all'inizio dell'apprendimento i valori Q stimati sono incerti, per l'agente è conveniente esplorare l'ambiente per raccogliere più esperienza. Con il passare del tempo l'agente è più sicuro del valore delle azioni quindi può abbassare l'esplorazione e continuare  a utilizzare i valori stimati. Questi metodi hanno due problemi principali. Primo, l'esplorazione è casuale e non diretta. In alcuni ambienti (come il gioco Montezuma's Revenge) le ricompense sono molto sparse. Serve una sequenza precisa di azioni per arrivare ad uno stato ``desiderato''. Basta un'azione sbagliata perché l'agente sia posizionato nello stato iniziale. Quindi è ovvio che un'esplorazione casuale non è sufficiente per risolvere questi tipi di problemi; serve un'esplorazione estesa o ``profonda''. Il secondo problema è che il livello di esplorazione si basa sulla durata dell'apprendimento e non sui valori stimati, o sull'ambiente specifico. Questo può causare una convergenza prematura a una politica subottimale. Infatti, uno dei problemi delle strategie come $\epsilon$-greedy è che devono essere studiati gli ``schedule'' di esplorazione per ogni problema specifico.\par
In questa tesi sviluppiamo un nuovo algoritmo di apprendimento model-free che si basa su lavori recenti che sostengono l'uso delle distribuzioni Q (Q-distributions) per guidare l'esplorazione. Modellando esplicitamente la distribuzione dei valori Q invece di valutare il valore medio, siamo in grado di prendere decisioni più consapevoli e utilizzare queste distribuzioni per guidare l'esplorazione. Partendo da una distribuzione precedente, possiamo aggiornare le nostre conoscenze con ogni nuovo campione dall'ambiente utilizzando un approccio bayesiano e possiamo anche utilizzare queste distribuzioni per quantificare il trade-off Exploration-Exploitation.\par
Il nostro algoritmo, chiamato Particle Q-learning, rappresenta la distribuzione dei Q-value usando un numero di posizioni variabili, che noi chiamiamo particelle. Ogni particella ha una probabilità fissa, e con l'apprendimento le particelle si muovono per rappresentare l'esperienza raccolta. In questo modo, chiamando $M$ il numero delle particelle, usiamo $M$ diverse stime del Q-value per ogni coppia stato-azione. Usiamo queste stime per calcolare l'incertezza sui Q value dell'agente. All'inizio dell'apprendimento le particelle sono sparse, per ogni coppia stato-azione. L'agente continua a raccogliere esperienza, e le particelle tendono a contrarsi verso il vero Q-value. Inoltre, in questa tesi, definiamo due politiche che usano queste distribuzioni di particelle, per scegliere  azioni. Una di queste, Weighted Policy, definisce la probabilità di esplorazione sulla base dell'incertezza dei Q-value. In questo modo possiamo usare l'incertezza sui Q-value per avere un'esplorazione più guidata e profonda, e allo stesso tempo, non usiamo euristiche per ``spegnere'' l'esplorazione. L'agente smetterà di esplorare quando sarà sicuro delle proprie stime dei Q-value.\par
Per testare l'algoritmo, iniziamo introducendo il nostro nuovo approccio in domini finiti semplici, progettati per enfatizzare l'esplorazione, per poi estenderlo a domini continui. Confrontiamo il nostro approccio con algoritmi allo stato dell'arte nei domini Taxi, Loop, Chain, SixArms, RiverSwim e KnightQuest, nonché in vari giochi Atari dall'Arcade Learning Environment.