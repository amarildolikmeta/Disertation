\par
The various sequential decision making problems are one object of study of Artificial Intelligence. Reinforcement learning addresses these problems in a trial and error way.  An agent is required to interact with an environment and collect experience from these interactions which in turn are used to find the optimal policy to pursue. One core element of reinforcement learning is the reward signal that the agent receives from the environment telling the agent if some states are desired or they should be avoided. This reward is assumed to be immediate after each action and the goal of the agent is to maximize the cumulative reward collected during its activity in the environment. Defined in this way, the reward function specifies the task to be learned by the agent.\par
The Exploitation- Exploration trade-off remains a main topic in reinforcement learning. The problem consists in balancing reward maximization using the knowledge acquired at the moment with exploring new actions to improve the knowledge of the environment. Traditionally exploration has been explicitly added to algorithms by occasionally choosing actions randomly instead of relying on the experience collected, nonetheless it remains a major challenge in reinforcement learning. Common exploration strategies, such as $\epsilon$-greedy, fail to conduct temporally-extended or deep exploration. This not only causes exponentially larger data requirements for the algorithms, but most importantly might cause premature convergence of the algorithms to a suboptimal policy or might prevent convergence altogether.\par
Traditionally reinforcement learning faces these problems by estimating the value function  which estimates how ``good'' the states are (or action-states pairs in the case of action-value function). Being that the agents interact with an ``uncertain'' environment the value-function is the expected cumulative reward collected in the long term. In this thesis we build on recent work advocating the use of Q-distributions to drive exploration. By explicitly modeling the distribution of the Q-values instead of just estimating the mean we are able to make more informed decisions and use these distributions to drive exploration. Starting from a prior distribution we can update our knowledge with each new sample using a Bayesian approach and we can also use these distributions to quantify the Exploration-Exploitation trade-off.\par
We start by introducing our new approach in simple finite domains, designed to emphasize exploration, for later extending it to continuous domains.   We compare our approach with state of the art algorithms in Taxi , Loop, Chain, SixArms, RiverSwim and KnightQuest domains as well as in various Atari games from the Arcade Learning Environment.