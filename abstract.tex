\par
Various sequential decision making problems are object of study of Artificial Intelligence. Reinforcement learning addresses these problems in a trial and error way.  An agent is left to interact with an environment and collect experience from these interactions which in turn are used to find the optimal policy to pursue. Core element of reinforcement learning is the reward signal  that the agent receives from the environment telling the agent if some states are preferred or they should be avoided. This reward is assumed to be immediate after each action and the goal of the agent is to maximize the cumulative rewa rd collected during his activity in the environment. Defined in this way, the reward function specifies the task to be learned by the agent.\par
The Exploitation- Exploration trade-off remains a main topic in reinforcement learning. The problem consists in balancing reward maximization using the knowledge acquired at the moment with exploring new actions to improve the knowledge of the environment. Traditionally exploration has been explicitly added to algorithms by occasionally choosing actions randomly instead of relying on the experience collected. Nonetheless exploration remains a major problem in reinforcement learning. Common exploration strategies, such as e-greedy, fail to conduct temporally-extended or deep exploration. This not only causes exponentially larger data requirements for the algorithms, but most importantly might cause premature convergence of the algorithms to a suboptimal policy or might prevent convergence altogether.\par
Traditionally reinforcement learning faces these problems by estimating the value function  which estimates how “good” the states are (or action-states pairs in the case of action-value function).Being that the agents interact with an “uncertain” environment the value-function is the expected cumulative reward collected in the long term. In this paper we build on recent work advocating a distributional approach on reinforcement learning. By explicitly modeling the distribution of the returns instead of just estimating the mean we are able to make more informed decisions and use these distributions to drive exploration. Starting from a prior distribution we can update our knowledge with each new sample using a Bayesian approach and we can also use these distributions to quantify the Exploration-Exploitation trade-off.\par
We start by introducing our new approach in simple finite domains, designed to emphasize exploration, for later extending it to continuous domains.   We compare our approach with state of the art algorithms in Taxi , Loop and Chain domains as well in various Atari games from the Arcade Learning Environment.